# Train Transformers

The Transformer — a neural network architecture introduced in 2017 by researchers at Google — has proved to be state-of-the-art, widely used in LLM models and subsequently now in many of multi-modal models along with diffusion.

In this implementation, a Transformer model is trained for language translation. The opus books dataset from Huggingface is used for training the model. This dataset supports language translation for many languages.

Thanks to Umar Jamil for creating a wonderful Transformer implementation tutorial on [YouTube](https://www.youtube.com/watch?v=ISNdQcPhsts)

## Architecture

![architecture](https://github.com/ra9hur/Train-Transformers/assets/17127066/77833160-d2b3-4d54-87c4-eacf6f0d613e)

## References

1. [Coding a Transformer from scratch on PyTorch, with full explanation, training and inference](https://www.youtube.com/watch?v=ISNdQcPhsts)

2. [A Visual Guide to Transformers Neural Networks: step by step explanation](https://www.youtube.com/playlist?list=PL86uXYUJ7999zE8u2-97i4KG_2Zpufkfb)

3. [Paper: Attention is All You Need](https://arxiv.org/abs/1706.03762)